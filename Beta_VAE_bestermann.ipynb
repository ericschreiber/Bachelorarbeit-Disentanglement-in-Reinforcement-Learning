{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cd44782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 42 x 42\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 1)  # 22 x 22\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 12 x 12\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 6 x 6\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(2304, 256) # 6x6x 64 = 2304\n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        \n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv4(h))\n",
    "        #print(h.size())\n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7847d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLikeDQN(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderLikeDQN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 10 x 10\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 10 x 10\n",
    "        self.flat1 = nn.Flatten()        \n",
    "        self.dense1 = nn.Linear(6400, 512) # 10x10x 64 = 6400\n",
    "        self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c85bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 2304)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, 2, 1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, 2, 1)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        \n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 6, 6)\n",
    "        h = self.act(self.upconv1(h))\n",
    "        h = self.act(self.upconv2(h))\n",
    "        h = self.act(self.upconv3(h))\n",
    "        img = self.upconv4(h)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1563d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLikeDQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderLikeDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        self.dense2 = nn.Linear(512, 6400)        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 10, 10)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        img = self.upconv3(h)\n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52b8a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, device, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        #self.encoder = EncoderLikeDQN(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        #self.decoder = DecoderLikeDQN(z_dim, num_channels, latent_dim)\n",
    "        self.encoder = Encoder(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        self.decoder = Decoder(z_dim, num_channels, latent_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        self.mse = 0\n",
    "        self.bce = 0\n",
    "        self.to(device)\n",
    "        #self.rec_loss = nn.MSELoss() try BCE Loss\n",
    "        self.rec_loss = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "       \n",
    "        \n",
    "    def num_channels(self):\n",
    "        return self.encoder.num_channels\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)\n",
    "        \n",
    "        mu = z[:, :, 0]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        reparam_z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = 0.5 * (sigma**2 + mu**2 - 2*torch.log(sigma) - 1).mean()\n",
    "        x_t = self.decoder(reparam_z).sigmoid()\n",
    "        #self.mse = self.rec_loss(x_t, x)\n",
    "        self.bce = self.rec_loss(x_t, x)\n",
    "        return x_t\n",
    "    \n",
    "    # TODO: Passe diese Klasse noch an. Vlt geht damit das Kopieren zurück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb022fe",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb50a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numpy.load('train_data100kFEB23.npy')\n",
    "val_data = numpy.load('val_data20kFEB23.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c95659",
   "metadata": {},
   "source": [
    "Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b492bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "latentDim = 64\n",
    "epochs = 10\n",
    "train_games = 100\n",
    "val_games = 20\n",
    "batch_size = 64\n",
    "kl_wheight = 1\n",
    "beta = 0\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ee69fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runLIKEDQNBeta{beta}Lat{latentDim}\" \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "savingDir = newpath + \"/outputBetaMAR14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57fa21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True, #this instructs DataLoader to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU.\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a960ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): EncoderLikeDQN(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "    (dense_means_logVar): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): DecoderLikeDQN(\n",
      "    (dense1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (dense2): Linear(in_features=512, out_features=6400, bias=True)\n",
      "    (upconv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (upconv2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv3): ConvTranspose2d(32, 1, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (rec_loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#enc = Encoder(latentDim, 1, latentDim).to(device)\n",
    "#dec = Decoder(latentDim, 1, latentDim).to(device)\n",
    "#optEnc = optim.Adam(enc.parameters(), lr=lr)\n",
    "#optDec = optim.Adam(dec.parameters(), lr=lr)\n",
    "\n",
    "#print(enc)\n",
    "#print(dec)\n",
    "\n",
    "vae = VAE(latentDim, 1, device, latentDim).to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "#criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52bb8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar, beta, kl_wheight):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the (one could also take the mse loss instead of bce then we get a kind of PCA)\n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta*kl_wheight*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8fb6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ce79f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "775e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fit(enc, dec, dataloader):\n",
    "def fit(vae, dataloader):\n",
    "    #enc.train()\n",
    "    #dec.train()\n",
    "    vae.train\n",
    "    running_loss = 0.0\n",
    "   # with torch.profiler.profile(schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=10),\n",
    "   #                             on_trace_ready=torch.profiler.tensorboard_trace_handler('C:/Users/erics/Documents/Programme/Bachelorarbeit/Profiler/BVAE/bestermann_MAR9_VAE_Class_Run8_runningLoss/'),\n",
    "   #                             record_shapes=True,\n",
    "   #                             profile_memory=True,\n",
    "   #                             with_stack=True) as prof: \n",
    "        \n",
    "   #     prof.start()\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        data = data.to(device)\n",
    "        data = data[:, None, :, :]\n",
    "        #optEnc.zero_grad()\n",
    "        #optDec.zero_grad()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "      #  interData = enc(data)\n",
    "      #  sample, mu, logvar = interData\n",
    "        #interData = reparameterize(mu, logvar)\n",
    "      #  reconstruction = dec(sample)\n",
    "        vae(data)        \n",
    "\n",
    "        #mse_loss = criterion(reconstruction, data)\n",
    "        #loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(train_data))\n",
    "        #kl_wheight = dataloader.batch_size/len(train_data) fixer hyperparameter\n",
    "        #loss = kl_wheight * beta * vae.kl + vae.mse\n",
    "        loss = kl_wheight * beta * vae.kl + vae.bce\n",
    "\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "        running_loss += loss.detach().cpu().numpy() # faster with detach().cpu().numpy() but double the copied amount just for plotting purposes\n",
    "        loss.backward()\n",
    "      #  optEnc.step()\n",
    "      #  optDec.step()\n",
    "        opt.step()\n",
    "\n",
    "       #     prof.step()\n",
    "       #     if(i > 100):\n",
    "       #         break\n",
    "       # prof.stop()\n",
    "        \n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316b4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def validate(enc, dec, dataloader):\n",
    "def validate(vae, dataloader):\n",
    "    #enc.eval()\n",
    "    #dec.eval()\n",
    "    vae.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data = data.to(device)\n",
    "            data = data[:, None, :, :]\n",
    "            \n",
    "          #  interData = enc(data)\n",
    "          #  sample, mu, logvar = interData\n",
    "          #  reconstruction = dec(sample)\n",
    "                \n",
    "          #  mse_loss = criterion(reconstruction, data)\n",
    "          #  loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(val_data))\n",
    "          #  running_loss += loss.item()\n",
    "            reconstruction = vae(data)        \n",
    "            kl_wheight = dataloader.batch_size/len(train_data)\n",
    "            loss = kl_wheight * beta * vae.kl + vae.mse\n",
    "            running_loss += loss.detach().cpu()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 1, 84, 84)[:8], \n",
    "                                  reconstruction.view(batch_size, 1, 84, 84)[:8]))\n",
    "                save_image(both.cpu(), savingDir + f\"{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adcba3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [00:40, 39.40it/s]                                                                                              \n",
      "328it [00:03, 84.76it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [00:43, 36.63it/s]                                                                                              \n",
      "328it [00:03, 87.32it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▏                                                        | 447/1591 [00:12<00:33, 34.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-8026ac15e87d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#val_epoch_loss = validate(enc, dec, val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-9df442285c2a>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(vae, dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#interData = reparameterize(mu, logvar)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m       \u001b[1;31m#  reconstruction = dec(sample)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#mse_loss = criterion(reconstruction, data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-ef5b024e94ff>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mreparam_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreparam_z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "torch.backends.cudnn.benchmark = True #choose best kernel for computation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    #train_epoch_loss = fit(enc, dec, train_loader)\n",
    "    #val_epoch_loss = validate(enc, dec, val_loader)\n",
    "    \n",
    "    train_epoch_loss = fit(vae, train_loader)\n",
    "    val_epoch_loss = validate(vae, val_loader)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0aac4",
   "metadata": {},
   "source": [
    "* Auslastung GPU: copy ~90%, vram 100% ( 2GB), 3D 0%\n",
    "* Auslastung CPU: ~25 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8c882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c119d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
