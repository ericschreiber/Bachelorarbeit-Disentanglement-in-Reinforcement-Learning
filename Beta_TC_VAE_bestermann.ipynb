{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cd44782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c16324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 42 x 42\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 1)  # 21 x 21\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 11 x 11\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 6 x 6\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(3136, 256) # 6x6x 64 = 2304\n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(\"conv1: \" + str(h.size()))\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(\"conv2: \" + str(h.size()))\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(\"conv3: \" + str(h.size()))\n",
    "        h = self.act(self.conv4(h))\n",
    "        #print(\"conv4: \" + str(h.size()))\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e6180178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLikeDQN(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderLikeDQN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 10 x 10\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 10 x 10\n",
    "        self.flat1 = nn.Flatten()        \n",
    "        self.dense1 = nn.Linear(6400, 512) # 10x10x 64 = 6400\n",
    "        self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a9c85bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 3136)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, stride=2, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, stride=2, padding=1)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"decoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(\"Transpose 1: \" + str(h.size()))\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(\"Transpose 2: \" + str(h.size()))\n",
    "        h = self.act(self.upconv3(h))\n",
    "        #print(\"Transpose 3: \" + str(h.size()))\n",
    "        img = self.upconv4(h)\n",
    "        #print(\"Transpose 4: \" + str(img.size()))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4e9f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLikeDQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderLikeDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        self.dense2 = nn.Linear(512, 6400)        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 10, 10)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        img = self.upconv3(h)\n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52b8a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, device, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        #self.encoder = EncoderLikeDQN(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        #self.decoder = DecoderLikeDQN(z_dim, num_channels, latent_dim)\n",
    "        self.encoder = Encoder(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        self.decoder = Decoder(z_dim, num_channels, latent_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        self.mse = 0\n",
    "        self.bce = 0\n",
    "        self.tc = 0\n",
    "        self.to(device)\n",
    "        #self.rec_loss = nn.MSELoss() try BCE Loss\n",
    "        self.rec_loss = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "    def gaussian_log_density(z_sampled: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_logvar: TensorType[\"batch\", \"num_latents\"]):\n",
    "        normalization = torch.log(torch.tensor(2. * np.pi))\n",
    "        inv_sigma = torch.exp(-z_logvar)\n",
    "        tmp = (z_sampled - z_mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + z_logvar + normalization)    \n",
    "\n",
    "    def total_correlation(z: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_logvar: TensorType[\"batch\", \"num_latents\"]) -> torch.Tensor:\n",
    "    \n",
    "        batch_size = z.size(0)\n",
    "        log_qz_prob = gaussian_log_density(z.unsqueeze(1), z_mean.unsqueeze(0), z_logvar.unsqueeze(0))\n",
    "\n",
    "        log_qz_product = torch.sum(\n",
    "            torch.logsumexp(log_qz_prob, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        log_qz = torch.logsumexp(\n",
    "            torch.sum(log_qz_prob, dim=2),\n",
    "            dim=1\n",
    "        )\n",
    "        return torch.mean(log_qz - log_qz_product)\n",
    "\n",
    "    \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "       \n",
    "        \n",
    "    def num_channels(self):\n",
    "        return self.encoder.num_channels\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)\n",
    "        \n",
    "        mu = z[:, :, 0]\n",
    "        logvar = z[:, :, 1]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        reparam_z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = 0.5 * (sigma**2 + mu**2 - 2*torch.log(sigma) - 1).mean()\n",
    "        self.tc = total_correlation(reparam_z, mu, logvar)\n",
    "        \n",
    "        x_t = self.decoder(reparam_z).sigmoid()\n",
    "        #self.mse = self.rec_loss(x_t, x)\n",
    "        self.bce = self.rec_loss(x_t, x)\n",
    "        return x_t\n",
    "    \n",
    "    # TODO: Passe diese Klasse noch an. Vlt geht damit das Kopieren zurück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb022fe",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb50a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numpy.load('train_data100kFEB23.npy')\n",
    "val_data = numpy.load('val_data20kFEB23.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c95659",
   "metadata": {},
   "source": [
    "Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b492bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "latentDim = 64\n",
    "epochs = 10\n",
    "train_games = 100\n",
    "val_games = 20\n",
    "batch_size = 64\n",
    "kl_wheight = 1\n",
    "tc_wheight = 1\n",
    "beta = 0\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0fedd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runLIKEDQNBeta{beta}Lat{latentDim}\"\n",
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/run1Beta{beta}Lat{latentDim}\"\n",
    "newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runConvTC{tc_wheight}_Beta{beta}Lat{latentDim}\"\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "savingDir = newpath + \"/outputBetaMAR14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57fa21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True, #this instructs DataLoader to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU.\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a960ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3136, out_features=256, bias=True)\n",
      "    (dense_means_logVar): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (dense2): Linear(in_features=256, out_features=3136, bias=True)\n",
      "    (upconv1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv3): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (rec_loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#enc = Encoder(latentDim, 1, latentDim).to(device)\n",
    "#dec = Decoder(latentDim, 1, latentDim).to(device)\n",
    "#optEnc = optim.Adam(enc.parameters(), lr=lr)\n",
    "#optDec = optim.Adam(dec.parameters(), lr=lr)\n",
    "\n",
    "#print(enc)\n",
    "#print(dec)\n",
    "\n",
    "vae = VAE(latentDim, 1, device, latentDim).to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "#criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52bb8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar, beta, kl_wheight):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the (one could also take the mse loss instead of bce then we get a kind of PCA)\n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta*kl_wheight*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8fb6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ce79f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "775e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fit(enc, dec, dataloader):\n",
    "def fit(vae, dataloader):\n",
    "    #enc.train()\n",
    "    #dec.train()\n",
    "    vae.train\n",
    "    running_loss = 0.0\n",
    "   # with torch.profiler.profile(schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=10),\n",
    "   #                             on_trace_ready=torch.profiler.tensorboard_trace_handler('C:/Users/erics/Documents/Programme/Bachelorarbeit/Profiler/BVAE/bestermann_MAR9_VAE_Class_Run8_runningLoss/'),\n",
    "   #                             record_shapes=True,\n",
    "   #                             profile_memory=True,\n",
    "   #                             with_stack=True) as prof: \n",
    "        \n",
    "   #     prof.start()\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        data = data.to(device)\n",
    "        data = data[:, None, :, :]\n",
    "        #optEnc.zero_grad()\n",
    "        #optDec.zero_grad()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "      #  interData = enc(data)\n",
    "      #  sample, mu, logvar = interData\n",
    "        #interData = reparameterize(mu, logvar)\n",
    "      #  reconstruction = dec(sample)\n",
    "        vae(data)        \n",
    "\n",
    "        #mse_loss = criterion(reconstruction, data)\n",
    "        #loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(train_data))\n",
    "        #kl_wheight = dataloader.batch_size/len(train_data) fixer hyperparameter\n",
    "        #loss = kl_wheight * beta * vae.kl + vae.mse\n",
    "        loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "        running_loss += loss.detach().cpu().numpy() # faster with detach().cpu().numpy() but double the copied amount just for plotting purposes\n",
    "        loss.backward()\n",
    "      #  optEnc.step()\n",
    "      #  optDec.step()\n",
    "        opt.step()\n",
    "\n",
    "       #     prof.step()\n",
    "       #     if(i > 100):\n",
    "       #         break\n",
    "       # prof.stop()\n",
    "        \n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316b4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def validate(enc, dec, dataloader):\n",
    "def validate(vae, dataloader):\n",
    "    #enc.eval()\n",
    "    #dec.eval()\n",
    "    vae.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data = data.to(device)\n",
    "            data = data[:, None, :, :]\n",
    "            \n",
    "          #  interData = enc(data)\n",
    "          #  sample, mu, logvar = interData\n",
    "          #  reconstruction = dec(sample)\n",
    "                \n",
    "          #  mse_loss = criterion(reconstruction, data)\n",
    "          #  loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(val_data))\n",
    "          #  running_loss += loss.item()\n",
    "            reconstruction = vae(data)        \n",
    "            kl_wheight = dataloader.batch_size/len(train_data)\n",
    "            loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "            \n",
    "            running_loss += loss.detach().cpu()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 1, 84, 84)[:8], \n",
    "                                  reconstruction.view(batch_size, 1, 84, 84)[:8]))\n",
    "                save_image(both.cpu(), savingDir + f\"{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "adcba3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [01:11, 22.32it/s]                                                                                              \n",
      "328it [00:09, 35.89it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:16, 11.66it/s]                                                                                              \n",
      "328it [00:13, 24.25it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:16, 11.68it/s]                                                                                              \n",
      "328it [00:13, 24.47it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:16, 11.69it/s]                                                                                              \n",
      "328it [00:13, 24.41it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:29, 10.68it/s]                                                                                              \n",
      "328it [00:13, 24.08it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:44,  9.71it/s]                                                                                              \n",
      "328it [00:15, 20.82it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:32, 10.44it/s]                                                                                              \n",
      "328it [00:15, 20.87it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [02:33, 10.35it/s]                                                                                              \n",
      "328it [00:15, 20.72it/s]                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101\n",
      "Val Loss: 0.0000\n",
      "Epoch 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                              | 31/1591 [00:03<02:41,  9.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-213-8026ac15e87d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#val_epoch_loss = validate(enc, dec, val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-9df442285c2a>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(vae, dataloader)\u001b[0m\n\u001b[0;32m     13\u001b[0m    \u001b[1;31m#     prof.start()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#optEnc.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "torch.backends.cudnn.benchmark = True #choose best kernel for computation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    #train_epoch_loss = fit(enc, dec, train_loader)\n",
    "    #val_epoch_loss = validate(enc, dec, val_loader)\n",
    "    \n",
    "    train_epoch_loss = fit(vae, train_loader)\n",
    "    val_epoch_loss = validate(vae, val_loader)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0aac4",
   "metadata": {},
   "source": [
    "* Auslastung GPU: copy ~90%, vram 100% ( 2GB), 3D 0%\n",
    "* Auslastung CPU: ~25 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8c882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c119d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
