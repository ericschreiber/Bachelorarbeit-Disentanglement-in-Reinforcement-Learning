{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875249a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd44782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "torch.set_printoptions(profile=\"full\") #print full tensor\n",
    "#torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9505b345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21961782e70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 0 \n",
    "numpy.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.random.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 42 x 42\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 1)  # 21 x 21\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 11 x 11\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 6 x 6\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(3136, 256) # 6x6x 64 = 2304\n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(\"conv1: \" + str(h.size()))\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(\"conv2: \" + str(h.size()))\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(\"conv3: \" + str(h.size()))\n",
    "        h = self.act(self.conv4(h))\n",
    "        #print(\"conv4: \" + str(h.size()))\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6180178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLikeDQN(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderLikeDQN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 10 x 10\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 10 x 10\n",
    "        self.flat1 = nn.Flatten()        \n",
    "        self.dense1 = nn.Linear(6400, 512) # 10x10x 64 = 6400\n",
    "        self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c85bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 3136)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, stride=2, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, stride=2, padding=1)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"decoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(\"Transpose 1: \" + str(h.size()))\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(\"Transpose 2: \" + str(h.size()))\n",
    "        h = self.act(self.upconv3(h))\n",
    "        #print(\"Transpose 3: \" + str(h.size()))\n",
    "        img = self.upconv4(h)\n",
    "        #print(\"Transpose 4: \" + str(img.size()))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e9f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLikeDQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderLikeDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        self.dense2 = nn.Linear(512, 6400)        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 10, 10)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        img = self.upconv3(h)\n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b8a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, device, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        self.encoder = EncoderLikeDQN(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        self.decoder = DecoderLikeDQN(z_dim, num_channels, latent_dim)\n",
    "        #self.encoder = Encoder(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        #self.decoder = Decoder(z_dim, num_channels, latent_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        self.mse = 0\n",
    "        self.bce = 0\n",
    "        self.tc = 0\n",
    "        self.to(device)\n",
    "        #self.rec_loss = nn.MSELoss() #try BCE Loss\n",
    "        self.rec_loss = nn.BCELoss()\n",
    "        #self.rec_loss = nn.BCEWithLogitsLoss() #clamp input values betweeen 0 & 1\n",
    "        \n",
    "        \n",
    "    def gaussian_log_density(self, z_sampled: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_logvar: TensorType[\"batch\", \"num_latents\"]):\n",
    "        normalization = torch.log(torch.tensor(2. * numpy.pi))\n",
    "        inv_sigma = torch.exp(-z_logvar)\n",
    "        tmp = (z_sampled - z_mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + z_logvar + normalization)    \n",
    "\n",
    "    def total_correlation(self, z: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_logvar: TensorType[\"batch\", \"num_latents\"]) -> torch.Tensor:\n",
    "    \n",
    "        batch_size = z.size(0)\n",
    "        log_qz_prob = self.gaussian_log_density(z.unsqueeze(1), z_mean.unsqueeze(0), z_logvar.unsqueeze(0))\n",
    "\n",
    "        log_qz_product = torch.sum(\n",
    "            torch.logsumexp(log_qz_prob, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        log_qz = torch.logsumexp(\n",
    "            torch.sum(log_qz_prob, dim=2),\n",
    "            dim=1\n",
    "        )\n",
    "        return torch.mean(log_qz - log_qz_product)\n",
    "\n",
    "    \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "       \n",
    "        \n",
    "    def num_channels(self):\n",
    "        return self.encoder.num_channels\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)\n",
    "        if torch.isnan(z).any():\n",
    "            print(\"z has NaN\")\n",
    "            print(z)\n",
    "            print(\"*************************************input saved***********\")\n",
    "            x = x.cpu().detach().numpy()\n",
    "            numpy.save( \"faulty_batch\", x)\n",
    "\n",
    "            \n",
    "            \n",
    "        mu = z[:, :, 0]\n",
    "        logvar = z[:, :, 1]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        reparam_z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = 0.5 * (sigma**2 + mu**2 - 2*torch.log(sigma) - 1).mean()\n",
    "        self.tc = self.total_correlation(reparam_z, mu, logvar)\n",
    "        \n",
    "        x_t = self.decoder(reparam_z).sigmoid()\n",
    "        #if torch.isnan(x_t).any():\n",
    "            #print(x_t)\n",
    "        #pred = x_t.clamp(0, 1) #push values between 0 and 1\n",
    "        #pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred) #vlt muss das noch rein\n",
    "        \n",
    "        #self.mse = self.rec_loss(x_t, x)\n",
    "        self.bce = self.rec_loss(x_t, x)\n",
    "        return x_t\n",
    "    \n",
    "    # TODO: Passe diese Klasse noch an. Vlt geht damit das Kopieren zurück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb022fe",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb50a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numpy.load('train_data100kMAR22.npy')\n",
    "val_data = numpy.load('val_data20kMAR22.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c95659",
   "metadata": {},
   "source": [
    "Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b492bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "latentDim = 64\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "kl_wheight = 0.00064\n",
    "beta = 3\n",
    "\n",
    "if beta != 0 :\n",
    "    tc_wheight = beta - 1\n",
    "else: \n",
    "    tc_wheight = 0\n",
    "beta = 1\n",
    "\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fedd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runLIKEDQNBeta{beta}Lat{latentDim}\"\n",
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/run1Beta{beta}Lat{latentDim}\"\n",
    "newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runConvTC{tc_wheight}_Beta{beta}Lat{latentDim}\"\n",
    "newpath = newpath + \"/outputBetaMAR28\"\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "savingDir = newpath + \"/epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57fa21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True, #this instructs DataLoader to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU.\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a960ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): EncoderLikeDQN(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "    (dense_means_logVar): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): DecoderLikeDQN(\n",
      "    (dense1): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (dense2): Linear(in_features=512, out_features=6400, bias=True)\n",
      "    (upconv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (upconv2): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv3): ConvTranspose2d(32, 1, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (rec_loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#enc = Encoder(latentDim, 1, latentDim).to(device)\n",
    "#dec = Decoder(latentDim, 1, latentDim).to(device)\n",
    "#optEnc = optim.Adam(enc.parameters(), lr=lr)\n",
    "#optDec = optim.Adam(dec.parameters(), lr=lr)\n",
    "\n",
    "#print(enc)\n",
    "#print(dec)\n",
    "\n",
    "vae = VAE(latentDim, 1, device, latentDim).to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "#criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52bb8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar, beta, kl_wheight):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the (one could also take the mse loss instead of bce then we get a kind of PCA)\n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta*kl_wheight*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8fb6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ce79f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fit(enc, dec, dataloader):\n",
    "def fit(vae, dataloader):\n",
    "    #enc.train()\n",
    "    #dec.train()\n",
    "    vae.train\n",
    "    running_loss = 0.0\n",
    "   # with torch.profiler.profile(schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=10),\n",
    "   #                             on_trace_ready=torch.profiler.tensorboard_trace_handler('C:/Users/erics/Documents/Programme/Bachelorarbeit/Profiler/BVAE/bestermann_MAR9_VAE_Class_Run8_runningLoss/'),\n",
    "   #                             record_shapes=True,\n",
    "   #                             profile_memory=True,\n",
    "   #                             with_stack=True) as prof: \n",
    "        \n",
    "   #     prof.start()\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        data = data.to(device)\n",
    "        data = data[:, None, :, :]\n",
    "\n",
    "        #optEnc.zero_grad()\n",
    "        #optDec.zero_grad()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        #opt.zero_grad()\n",
    "\n",
    "      #  interData = enc(data)\n",
    "      #  sample, mu, logvar = interData\n",
    "        #interData = reparameterize(mu, logvar)\n",
    "      #  reconstruction = dec(sample)\n",
    "        vae(data)        \n",
    "\n",
    "        #mse_loss = criterion(reconstruction, data)\n",
    "        #loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(train_data))\n",
    "        #kl_wheight = dataloader.batch_size/len(train_data) fixer hyperparameter\n",
    "        #loss = kl_wheight * beta * vae.kl + vae.mse\n",
    "        loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "        \n",
    "\n",
    "        #running_loss += loss.item()\n",
    "        running_loss += loss.detach().cpu().numpy() # faster with detach().cpu().numpy() but double the copied amount just for plotting purposes\n",
    "        loss.backward()\n",
    "      #  optEnc.step()\n",
    "      #  optDec.step()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.5, norm_type=2.0) #args.clip) #clipping gradient\n",
    "        opt.step()\n",
    "        #print(opt.param_groups[0]['lr'])\n",
    "\n",
    "       #     prof.step()\n",
    "       #     if(i > 100):\n",
    "       #         break\n",
    "       # prof.stop()\n",
    "        \n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316b4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def validate(enc, dec, dataloader):\n",
    "def validate(vae, dataloader):\n",
    "    #enc.eval()\n",
    "    #dec.eval()\n",
    "    vae.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data = data.to(device)\n",
    "            data = data[:, None, :, :]\n",
    "            \n",
    "          #  interData = enc(data)\n",
    "          #  sample, mu, logvar = interData\n",
    "          #  reconstruction = dec(sample)\n",
    "                \n",
    "          #  mse_loss = criterion(reconstruction, data)\n",
    "          #  loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(val_data))\n",
    "          #  running_loss += loss.item()\n",
    "            reconstruction = vae(data)        \n",
    "            kl_wheight = dataloader.batch_size/len(train_data)\n",
    "            loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "            \n",
    "            running_loss += loss.detach().cpu()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 1, 84, 84)[:8], \n",
    "                                  reconstruction.view(batch_size, 1, 84, 84)[:8]))\n",
    "                save_image(both.cpu(), savingDir + f\"{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adcba3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                | 6/1562 [00:04<12:58,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                               | 14/1562 [00:05<03:56,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                               | 18/1562 [00:05<02:42,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                              | 26/1562 [00:05<01:34, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                              | 34/1562 [00:05<01:09, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▏                                                                             | 42/1562 [00:05<00:54, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▎                                                                             | 46/1562 [00:06<00:51, 29.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                             | 54/1562 [00:06<00:48, 30.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                            | 62/1562 [00:06<00:46, 32.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                            | 70/1562 [00:06<00:43, 34.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▋                                                                            | 73/1562 [00:06<02:19, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n",
      "data size: \n",
      "torch.Size([64, 1, 84, 84])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8026ac15e87d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#val_epoch_loss = validate(enc, dec, val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-44980ccdd32e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(vae, dataloader)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#running_loss += loss.item()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# faster with detach().cpu().numpy() but double the copied amount just for plotting purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m       \u001b[1;31m#  optEnc.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[1;31m#  optDec.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "torch.backends.cudnn.benchmark = True #choose best kernel for computation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    #train_epoch_loss = fit(enc, dec, train_loader)\n",
    "    #val_epoch_loss = validate(enc, dec, val_loader)\n",
    "    \n",
    "    train_epoch_loss = fit(vae, train_loader)\n",
    "    val_epoch_loss = validate(vae, val_loader)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0aac4",
   "metadata": {},
   "source": [
    "* Auslastung GPU: copy ~90%, vram 100% ( 2GB), 3D 0%\n",
    "* Auslastung CPU: ~25 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57af84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67aa871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_batch = numpy.load('faulty_batch.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db27d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "    all_zeros = not faulty_batch[i].any()\n",
    "    print(all_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f352bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGgCAYAAAAD9NhnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkTElEQVR4nO3df1Dc9YH/8deHYwObSHZJIgIHRBBCpJdg1Dt6lTOxLc13CDPcOg6Tes6ZCY5WmHbO+Tqe15WOVixfvJ6m47XX5kibdhxNA7LFEUaTML0a9CaZeufQlg42YibGaAgTlx8bdlnKfv/I5XNSksiy+/EdyPMx43Cf9+f9+ex7X8P0de9ld2PFYrGYAAAwIMX0AgAAVy9KCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgTKpTN37ttdf08ssvKxgMKi8vTzt27NCNN97o1MMBABYhR3ZCb775pvbu3as777xTra2tuvHGG/Wd73xHIyMjTjwcAGCRcqSEXnnlFX3xi1/Ul770JXsXtGbNGh04cMCJhwMALFJJfzluenpaQ0ND+tu//dtZ4xs3btTg4OCc+dFoVNFo1D62LEtut1v/ct8PdObkWf2/Vx/To/+nWZFzkWQv9aqXtjyNfB1Evs4iX2clkm/euhz937aGec21kv1POZw9e1Zf+9rX9OSTT6q0tNQe7+zs1K9+9St973vfmzV///796ujosI8LCwvV2tqazCUBAK5Qjr0xwbKseY35fD7V1NTMmfPQ7U36cOi09p3cre1592tyPOzUUq9a7ox08nUQ+TqLfJ2VSL5F5Wv17OtPzmtu0kto5cqVSklJUTAYnDU+Ojoqj8czZ77L5ZLL5ZozHg5F7Cc+OR7WufHJZC8V/4N8nUW+ziJfZy0k33Bo/i/fJf2NCampqSoqKlJ/f/+s8f7+/lkvzwEA4MjLcTU1NXruuedUVFSkdevW6dChQxoZGVFVVZUTDwcAWKQcKaEvfOELGh8f10svvaSPP/5Y+fn5+qd/+idde+21TjwcAGCRcuyNCVu3btXWrVuduj0AYAngu+MAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMakxnvBwMCAXn75Zb333nv6+OOP9fDDD+uv/uqv7POxWEzt7e3q7e3VxMSESkpKVF9fr/z8/KQuHACw+MW9E4pEIrr++uu1c+fOi57v6upSd3e3du7cqZaWFnm9XjU3N2tycjLhxQIAlpa4S2jTpk3avn27Kioq5pyLxWLq6emRz+dTRUWFCgoK1NjYqEgkor6+vqQsGACwdMT9ctzlDA8PKxgMqry83B5zuVwqKyvT4OCgqqqq5lwTjUYVjUbtY8uy5Ha7lb4iTe6MdEmyfyK5yNdZ5Oss8nVWIvmmr0ib99ykllAwGJQkeTyeWeMej0cjIyMXvSYQCKijo8M+LiwsVGtrq559/Ul7bN/J3clcJv4E+TqLfJ1Fvs5yOt+kltAFlmXNOo7FYpec6/P5VFNTM+fah25v0odDp7Xv5G5tz7tfk+NhJ5Z6VXNnpJOvg8jXWeTrrETyLSpfO2sjcTlJLSGv1yvp/I4oMzPTHh8bG5uzO7rA5XLJ5XLNGQ+HIvYTnxwP69w4b2xwCvk6i3ydRb7OWki+4VBk3nOT+jmhrKwseb1e9ff322PT09MaGBhQaWlpMh8KALAExL0TCofD+uijj+zj4eFhHT9+XNdcc43WrFmj6upqBQIB5eTkKDs7W4FAQGlpaaqsrEzqwgEAi1/cJfTuu+/qiSeesI9/9rOfSZI2b96sxsZG1dbWampqSm1tbQqFQiouLpbf75fb7U7eqgEAS0LcJfS5z31O+/fvv+R5y7JUV1enurq6hBYGAFj6+O44AIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMSTW9AADAXK+dejvua7bm3pT0dTiNnRAAwBhKCABgTFwvxwUCAR09elQffPCBli1bpnXr1umee+5Rbm6uPScWi6m9vV29vb2amJhQSUmJ6uvrlZ+fn/TFAwAWt7h2QgMDA9q6daueeuopPfbYY5qZmVFzc7PC4bA9p6urS93d3dq5c6daWlrk9XrV3NysycnJpC8eALC4xVVCfr9fW7ZsUX5+vq6//no1NDRoZGREQ0NDks7vgnp6euTz+VRRUaGCggI1NjYqEomor6/PkScAAFi8Enp33Llz5yRJ11xzjSRpeHhYwWBQ5eXl9hyXy6WysjINDg6qqqpqzj2i0aii0ah9bFmW3G630lekyZ2RLkn2TyQX+TqLfJ215PO1ron7kuUZ7qQ9fCL5pq9Im/fcBZdQLBbTT3/6U61fv14FBQWSpGAwKEnyeDyz5no8Ho2MjFz0PoFAQB0dHfZxYWGhWltb9ezrT9pj+07uXugyMQ/k6yzydRb5/q+u0eTf0+l8F1xCe/bs0YkTJ/Ttb397zjnLsmYdx2KxS97H5/OppqZmzrUP3d6kD4dOa9/J3dqed78mx8OXugUWyJ2RTr4OIl9nLfV8A+/8Ju5rfOs2JO3xE8m3qHztrI3E5SyohH784x/rrbfe0hNPPKHVq1fb416vV9L5HVFmZqY9PjY2Nmd3dIHL5ZLL5ZozHg5F7Cc+OR7WuXHe2OAU8nUW+TpryeYbm4j7EidyWEi+4VBk3nPjemNCLBbTnj17dOTIEX3rW99SVlbWrPNZWVnyer3q7++3x6anpzUwMKDS0tJ4HgoAcBWIaye0Z88e9fX16ZFHHpHb7bb/BrR8+XItW7ZMlmWpurpagUBAOTk5ys7OViAQUFpamiorK51YPwBgEYurhA4cOCBJevzxx2eNNzQ0aMuWLZKk2tpaTU1Nqa2tTaFQSMXFxfL7/XK7k/euDQDA0hBXCe3fv/9T51iWpbq6OtXV1S14UQCAqwPfHQcAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGpphcAAJhra+5NppfwmWAnBAAwhhICABgT18txBw4c0IEDB3TmzBlJUl5enu666y5t2rRJkhSLxdTe3q7e3l5NTEyopKRE9fX1ys/PT/7KAQCLXlw7oVWrVunuu+9WS0uLWlpa9Bd/8Rd6+umn9f7770uSurq61N3drZ07d6qlpUVer1fNzc2anJx0ZPEAgMUtrhK69dZbdfPNNys3N1e5ubn66le/qvT0dP3hD39QLBZTT0+PfD6fKioqVFBQoMbGRkUiEfX19Tm1fgDAIrbgd8fNzMzoP//zPxWJRLRu3ToNDw8rGAyqvLzcnuNyuVRWVqbBwUFVVVVd9D7RaFTRaNQ+tixLbrdb6SvS5M5IlyT7J5KLfJ1Fvs4iX2clkm/6irR5z427hE6cOCG/369oNKr09HQ9/PDDysvL0+DgoCTJ4/HMmu/xeDQyMnLJ+wUCAXV0dNjHhYWFam1t1bOvP2mP7Tu5O95lIg7k6yzydRb5OsvpfOMuodzcXP3zP/+zQqGQjhw5ou9///t64okn7POWZc2aH4vFLns/n8+nmpqaOdc/dHuTPhw6rX0nd2t73v2aHA/Hu1R8CndGOvk6iHydRb7OSiTfovK1szYSlxN3CaWmpio7O1uSdMMNN+jdd99VT0+PamtrJUnBYFCZmZn2/LGxsTm7o09yuVxyuVxzxsOhiP3EJ8fDOjfOmxucQr7OIl9nka+zFpJvOBSZ99yEPycUi8UUjUaVlZUlr9er/v5++9z09LQGBgZUWlqa6MMAAJaguHZCL7zwgjZt2qTVq1crHA7rjTfe0O9+9zv5/X5ZlqXq6moFAgHl5OQoOztbgUBAaWlpqqysdGr9AIBFLK4SGh0d1b/+67/q448/1vLly7V27Vr5/X5t3LhRklRbW6upqSm1tbUpFAqpuLhYfr9fbrfbkcUDABa3uErowQcfvOx5y7JUV1enurq6hBYFALg68N1xAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjUhO5OBAI6MUXX1R1dbV27NghSYrFYmpvb1dvb68mJiZUUlKi+vp65efnJ2O9AIAlZME7oWPHjunQoUNau3btrPGuri51d3dr586damlpkdfrVXNzsyYnJxNeLABgaVlQCYXDYT333HN64IEHtGLFCns8Foupp6dHPp9PFRUVKigoUGNjoyKRiPr6+pK2aADA0rCgl+Pa2tq0adMmbdy4UZ2dnfb48PCwgsGgysvL7TGXy6WysjINDg6qqqpqzr2i0aii0ah9bFmW3G630lekyZ2RLkn2TyQX+TqLfJ1Fvs5KJN/0FWnznht3Cb3xxht677331NLSMudcMBiUJHk8nlnjHo9HIyMjF71fIBBQR0eHfVxYWKjW1lY9+/qT9ti+k7vjXSbiQL7OIl9nka+znM43rhIaGRnR3r175ff7tWzZskvOsyxr1nEsFrvkXJ/Pp5qamjnXPnR7kz4cOq19J3dre979mhwPx7NUzIM7I518HUS+ziJfZyWSb1H52lkbicuJq4SGhoY0OjqqRx991B6bmZnR73//e7366qvatWuXpPM7oszMTHvO2NjYnN3RBS6XSy6Xa854OBSxn/jkeFjnxnljg1PI11nk6yzyddZC8g2HIvOeG1cJbdiwQd/97ndnjf3bv/2bcnNzVVtbq+uuu05er1f9/f0qLCyUJE1PT2tgYEB/93d/F89DAQCuAnGVkNvtVkFBwayxtLQ0ZWRk2OPV1dUKBALKyclRdna2AoGA0tLSVFlZmbxVAwCWhIQ+rHoxtbW1mpqaUltbm0KhkIqLi+X3++V2u5P9UACARS7hEnr88cdnHVuWpbq6OtXV1SV6awDAEsd3xwEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjEmNZ/L+/fvV0dExa8zj8ejf//3fJUmxWEzt7e3q7e3VxMSESkpKVF9fr/z8/OStGACwZMRVQpKUn5+vpqYm+zgl5X83U11dXeru7lZDQ4NycnLU2dmp5uZm7dq1S263OzkrBgAsGXG/HJeSkiKv12v/t3LlSknnd0E9PT3y+XyqqKhQQUGBGhsbFYlE1NfXl/SFAwAWv7h3Qh999JEeeOABpaamqqSkRF/96ld13XXXaXh4WMFgUOXl5fZcl8ulsrIyDQ4Oqqqq6qL3i0ajikaj9rFlWXK73UpfkSZ3Rrok2T+RXOTrLPJ1Fvk6K5F801ekzXuuFYvFYvOd/N///d+KRCLKzc1VMBhUZ2enPvjgAz3zzDM6deqUmpqa9MMf/lCrVq2yr/nRj36kkZER+f3+i97zT//OVFhYqNbW1nk/AQDA4hXXTmjTpk32/11QUKB169bp61//un71q1+ppKRE0vmdzCd9Wsf5fD7V1NTYxxeuf+j2Jn04dFr7Tu7W9rz7NTkejmepmAd3Rjr5Ooh8nUW+zkok36LytXr29SfnNTful+M+KT09XQUFBfrwww/1l3/5l5KkYDCozMxMe87Y2Jg8Hs8l7+FyueRyueaMh0MR+4lPjod1bnwykaXiMsjXWeTrLPJ11kLyDYci856b0OeEotGoPvjgA2VmZiorK0ter1f9/f32+enpaQ0MDKi0tDSRhwEALFFx7YR+9rOf6dZbb9WaNWs0Ojqql156SZOTk9q8ebMsy1J1dbUCgYBycnKUnZ2tQCCgtLQ0VVZWOrV+AMAiFlcJnT17Vt/73vc0NjamlStXqqSkRE899ZSuvfZaSVJtba2mpqbU1tamUCik4uJi+f1+PiMEALiouEroH/7hHy573rIs1dXVqa6uLpE1AQCuEnx3HADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAGEoIAGAMJQQAMIYSAgAYQwkBAIyhhAAAxlBCAABjKCEAgDGUEADAmNR4Lzh79qyef/55vf3225qamlJOTo4efPBBFRUVSZJisZja29vV29uriYkJlZSUqL6+Xvn5+UlfPABgcYurhCYmJtTU1KTPfe5z+uY3v6mVK1fq9OnTWr58uT2nq6tL3d3damhoUE5Ojjo7O9Xc3Kxdu3bJ7XYn/Qng07126u2Ln7CukSQF3vmNFJuYdWpr7k3OLgoAFOfLcV1dXVq9erUaGhpUXFysrKwsbdiwQdnZ2ZLO74J6enrk8/lUUVGhgoICNTY2KhKJqK+vz5EnAABYvOLaCf36179WeXm5nnnmGQ0MDGjVqlX6yle+oi9/+cuSpOHhYQWDQZWXl9vXuFwulZWVaXBwUFVVVXPuGY1GFY1G7WPLsuR2u5W+Ik3ujHRJsn9igf5nxzN3fMXsn5+wPINda6L4/XUW+TorkXzTV6TNe25cJTQ8PKyDBw9q27Zt8vl8OnbsmH7yk5/I5XJp8+bNCgaDkiSPxzPrOo/Ho5GRkYveMxAIqKOjwz4uLCxUa2urnn39SXts38nd8SwTcUrJmrtL7Ro1sJAlit9fZ5Gvs5zON64SmpmZ0Q033KC7775b0vnCeP/993XgwAFt3rzZnmdZ1qzrYrHYJe/p8/lUU1Mz59qHbm/Sh0Onte/kbm3Pu1+T4+F4lopPCLzzm4ufsFYoJatPM8OVUiw065Rv3YbPYGVLmzsjnd9fB5GvsxLJt6h87ayNxOXEVUKZmZnKy8ubNZaXl6cjR45IkrxeryQpGAwqMzPTnjM2NjZnd3SBy+WSy+WaMx4ORewnPjke1rnxyXiWik/6kzcdzD0fmjOHvJOH319nka+zFpJvOBSZ99y43phQWlqqU6dOzRo7deqUrr32WklSVlaWvF6v+vv77fPT09MaGBhQaWlpPA8FALgKxFVC27Zt0x/+8Ad1dnbqo48+Ul9fn3p7e7V161ZJ519Kq66uViAQ0NGjR3XixAl9//vfV1pamiorKx15AgCAxSuul+OKi4v18MMP64UXXtBLL72krKws3Xvvvfqbv/kbe05tba2mpqbU1tamUCik4uJi+f1+PiMEAJgj7m9MuOWWW3TLLbdc8rxlWaqrq1NdXV1CCwMALH18dxwAwBhKCABgDCUEADAm7r8JYfG51JeRLs9wq2v0/AdT+ZwFABPYCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMCY1HgmNzY26syZM3PGv/KVr+i+++5TLBZTe3u7ent7NTExoZKSEtXX1ys/Pz9pCwYALB1xlVBLS4tmZmbs4xMnTqi5uVl//dd/LUnq6upSd3e3GhoalJOTo87OTjU3N2vXrl1yu93JXTkAYNGL6+W4lStXyuv12v/913/9l6677jqVlZUpFoupp6dHPp9PFRUVKigoUGNjoyKRiPr6+pxaPwBgEYtrJ/RJ09PTOnz4sLZt2ybLsnT69GkFg0GVl5fbc1wul8rKyjQ4OKiqqqqL3icajSoajdrHlmXJ7XYrfUWa3BnpkmT/RHKRr7PI11nk66xE8k1fkTbvuQsuoaNHjyoUCmnLli2SpGAwKEnyeDyz5nk8Ho2MjFzyPoFAQB0dHfZxYWGhWltb9ezrT9pj+07uXugyMQ/k6yzydRb5OsvpfBdcQr/85S910003adWqVbPGLcuadRyLxS57H5/Pp5qamjnXP3R7kz4cOq19J3dre979mhwPL3SpuAR3Rjr5Ooh8nUW+zkok36LytbM2EpezoBI6c+aM+vv79fDDD9tjXq9X0vkdUWZmpj0+NjY2Z3f0SS6XSy6Xa854OBSxn/jkeFjnxicXslTMA/k6i3ydRb7OWki+4VBk3nMX9DmhX/7yl/J4PLr55pvtsaysLHm9XvX399tj09PTGhgYUGlp6UIeBgCwxMW9E5qZmdF//Md/aPPmzfqzP/sze9yyLFVXVysQCCgnJ0fZ2dkKBAJKS0tTZWVlUhcNAFga4i6h3/zmNxoZGdEdd9wx51xtba2mpqbU1tamUCik4uJi+f1+PiMEALiouEuovLxc+/fvv+g5y7JUV1enurq6hBcGAFj6+O44AIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxlBAAwBhKCABgDCUEADCGEgIAGEMJAQCMoYQAAMZQQgAAYyghAIAxqfFM/uMf/6j29nYdPnxYwWBQmZmZ2rJli+68806lpJzvs1gspvb2dvX29mpiYkIlJSWqr69Xfn6+I08AALB4xVVCXV1dOnjwoBobG5WXl6ehoSH94Ac/0PLly1VdXW3P6e7uVkNDg3JyctTZ2anm5mbt2rVLbrfbkScBAFic4no57p133tGtt96qm2++WVlZWfr85z+vjRs36t1335V0fhfU09Mjn8+niooKFRQUqLGxUZFIRH19fY48AQDA4hXXTmj9+vU6ePCgTp06pdzcXB0/flyDg4O69957JUnDw8MKBoMqLy+3r3G5XCorK9Pg4KCqqqrm3DMajSoajdrHlmXJ7XYrfUWa3BnpkmT/RHKRr7PI11nk66xE8k1fkTbvuVYsFovNd3IsFtOLL76orq4upaSkaGZmRtu3b5fP55MkDQ4OqqmpST/84Q+1atUq+7of/ehHGhkZkd/vn3PP/fv3q6Ojwz4uLCxUa2vrvJ8AAGDximsn9Oabb+rw4cP6xje+ofz8fB0/flx79+6136BwgWVZs667XM/5fD7V1NTMufah25v04dBp7Tu5W9vz7tfkeDiepWIe3Bnp5Osg8nUW+TorkXyLytfq2defnNfcuEro+eefV21trW677TZJUkFBgc6cOaNf/OIX2rJli7xeryTZ75y7YGxsTB6P56L3dLlccrlcc8bDoYj9xCfHwzo3PhnPUpPmtVNvx33N1tybkr4OJ5nM92pAvs4iX2ctJN9wKDLvuXG9MSESidhvxbZvkJJi73SysrLk9XrV399vn5+entbAwIBKS0vjeSgAwFUgrp3QLbfcos7OTq1Zs0Z5eXk6fvy4XnnlFd1xxx2Szr+UVl1drUAgoJycHGVnZysQCCgtLU2VlZWOPAEAwOIVVwnt3LlTP//5z9XW1qbR0VGtWrVKVVVVuuuuu+w5tbW1mpqaUltbm0KhkIqLi+X3+/mMEABgjrhKyO12a8eOHdqxY8cl51iWpbq6OtXV1SW6NgDAEsd3xwEAjKGEAADGUEIAAGMoIQCAMZQQAMAYSggAYAwlBAAwJq7PCX2W8tf/uf114EXla+P6LqKkSp2K+5LiTYUOLCT5roh8lzDydRb5OiuRfPPX//m858b1TzkAAJBMV/TLcZOTk/rHf/xHTU7yDblOIF9nka+zyNdZn1W+V3QJxWIxvffee5f994iwcOTrLPJ1Fvk667PK94ouIQDA0kYJAQCMuaJLyOVy6a677rrov7yKxJGvs8jXWeTrrM8qX94dBwAw5oreCQEAljZKCABgDCUEADCGEgIAGEMJAQCMuWK/wPS1117Tyy+/rGAwqLy8PO3YsUM33nij6WUtOoFAQEePHtUHH3ygZcuWad26dbrnnnuUm5trz4nFYmpvb1dvb68mJiZUUlKi+vp65efnG1z54hMIBPTiiy+qurpaO3bskES2yXD27Fk9//zzevvttzU1NaWcnBw9+OCDKioqkkTGifjjH/+o9vZ2HT58WMFgUJmZmdqyZYvuvPNOpaSc36M4ne8VuRN68803tXfvXt15551qbW3VjTfeqO985zsaGRkxvbRFZ2BgQFu3btVTTz2lxx57TDMzM2publY4HLbndHV1qbu7Wzt37lRLS4u8Xq+am5v5Tq44HDt2TIcOHdLatWtnjZNtYiYmJtTU1KTU1FR985vf1DPPPKO///u/1/Lly+05ZLxwXV1dOnjwoOrr6/Xss8/qnnvu0csvv6xXX3111hwn870iS+iVV17RF7/4RX3pS1+yd0Fr1qzRgQMHTC9t0fH7/dqyZYvy8/N1/fXXq6GhQSMjIxoaGpJ0/v/L6enpkc/nU0VFhQoKCtTY2KhIJKK+vj7Dq18cwuGwnnvuOT3wwANasWKFPU62ievq6tLq1avV0NCg4uJiZWVlacOGDcrOzpZExol65513dOutt+rmm29WVlaWPv/5z2vjxo169913JX02+V5xJTQ9Pa2hoSGVl5fPGt+4caMGBwcNrWrpOHfunCTpmmuukSQNDw8rGAzOytvlcqmsrIy856mtrU2bNm3Sxo0bZ42TbeJ+/etfq6ioSM8884zuu+8+PfLIIzp06JB9nowTs379ev32t7/VqVOnJEnHjx/X4OCgNm3aJOmzyfeK+5vQ2NiYZmZm5PF4Zo17PB4Fg0Ezi1oiYrGYfvrTn2r9+vUqKCiQJDvTi+XNy5+f7o033tB7772nlpaWOefINnHDw8M6ePCgtm3bJp/Pp2PHjuknP/mJXC6XNm/eTMYJqq2t1blz5/TQQw8pJSVFMzMz2r59uyorKyV9Nr/DV1wJXWBZ1rzGMH979uzRiRMn9O1vf3vOuT/Nlm9z+nQjIyPau3ev/H6/li1bdsl5ZLtwMzMzuuGGG3T33XdLkgoLC/X+++/rwIED2rx5sz2PjBfmzTff1OHDh/WNb3xD+fn5On78uPbu3Wu/QeECJ/O94kpo5cqVSklJmbPrGR0dndPGmL8f//jHeuutt/TEE09o9erV9rjX65Uk+50xF4yNjZH3pxgaGtLo6KgeffRRe2xmZka///3v9eqrr2rXrl2SyDYRmZmZysvLmzWWl5enI0eOSOL3N1HPP/+8amtrddttt0mSCgoKdObMGf3iF7/Qli1bPpN8r7i/CaWmpqqoqEj9/f2zxvv7+1VaWmpoVYtXLBbTnj17dOTIEX3rW99SVlbWrPNZWVnyer2z8p6entbAwAB5f4oNGzbou9/9rp5++mn7vxtuuEGVlZV6+umndd1115FtgkpLS+2/V1xw6tQpXXvttZL4/U1UJBKx34p9QUpKir3T+SzyveJ2QpJUU1Oj5557TkVFRVq3bp0OHTqkkZERVVVVmV7aorNnzx719fXpkUcekdvttneYy5cv17Jly2RZlqqrqxUIBJSTk6Ps7GwFAgGlpaXZrwvj4txut/23tQvS0tKUkZFhj5NtYrZt26ampiZ1dnbqC1/4go4dO6be3l7df//9ksTvb4JuueUWdXZ2as2aNcrLy9Px48f1yiuv6I477pD02eR7xf5TDhc+rPrxxx8rPz9f9957r8rKykwva9Gpq6u76HhDQ4P9mu+FD6MdOnRIoVBIxcXFqq+vn/M/sPh0jz/+uK6//vo5H1Yl24V766239MILL+ijjz5SVlaWtm3bpi9/+cv2eTJeuMnJSf385z/X0aNHNTo6qlWrVum2227TXXfdpdTU83sUp/O9YksIALD0XXF/EwIAXD0oIQCAMZQQAMAYSggAYAwlBAAwhhICABhDCQEAjKGEAADGUEIAAGMoIQCAMZQQAMCY/w9C+aYyHcm+TwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(faulty_batch[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
