{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n",
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "bde4eb63-b316-46d9-cb3c-0d5b2f066da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8QDaXip14JBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1uzLQLz04z2i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VjUM99rEKFNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  1 11:46:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 497.09       Driver Version: 497.09       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A    0C    P8    N/A /  N/A |     37MiB /  2048MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "BVAEtoDevice = False #should prepro happen on GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "features = 64\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "   #     self.conv = nn.Sequential(\n",
    "   #         nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "   #         nn.ReLU(),\n",
    "   #         nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "   #         nn.ReLU(),\n",
    "   #         nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "   #         nn.ReLU()\n",
    "   #     )\n",
    "        \n",
    "        \n",
    "        #conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.Linear(conv_out_size, 512),\n",
    "            nn.Linear(features*2*4, 512), #buffer von 4 Bilder nacheinander als input (features*2 kommt vom training vom VAE)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    #def _get_conv_out(self, shape):\n",
    "    #    o = self.conv(torch.zeros(1, *shape))\n",
    "    #    return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        #return self.fc(conv_out)\n",
    "        #print(x.size())\n",
    "        return self.fc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "taYi5LZnIOqz",
    "outputId": "c96d3a1d-ebf6-471a-a93f-c96b479cc9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained BVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 64\n",
    "# define a simple linear VAE #until now normal VAE without Beta\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearVAE, self).__init__()\n",
    " \n",
    "        # encoder 84*84 = 7’056\n",
    "        self.enc0 = nn.Linear(in_features=84*84, out_features=1024)\n",
    "        self.enc1 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.enc2 = nn.Linear(in_features=512, out_features=features*2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec0 = nn.Linear(in_features=features, out_features=512)\n",
    "        self.dec1 = nn.Linear(in_features=512, out_features=1024)\n",
    "        self.dec2 = nn.Linear(in_features=1024, out_features=84*84)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    " #   def forward(self, x):\n",
    " #       # encoding\n",
    " #       x = F.relu(self.enc0(x))\n",
    " #       x = F.relu(self.enc1(x))\n",
    "\n",
    " #       x = self.enc2(x).view(-1, 2, features)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    " #       mu = x[:, 0, :] # the first feature values as mean\n",
    " #       log_var = x[:, 1, :] # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    " #       z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    " #       x = F.relu(self.dec0(z))\n",
    " #       x = F.relu(self.dec1(x))\n",
    " #       reconstruction = torch.sigmoid(self.dec2(x))\n",
    " #       return reconstruction, mu, log_var\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.enc0(x))\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.enc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BVAE = LinearVAE()\n",
    "BVAE.load_state_dict(torch.load('C:/Users/erics/Documents/Programme/Bachelorarbeit/models/BVAE_Pong/B=10VAEFEB25'))\n",
    "if BVAEtoDevice == True:\n",
    "    BVAE.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "VISUALIZEtraining = True\n",
    "MEAN_REWARD_BOUND = -19   #Change to 19.0    \n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, BVAE, net, epsilon=0.0, device=\"cpu\"):\n",
    "        \n",
    "        if VISUALIZEtraining:\n",
    "            env.render()\n",
    "            \n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a)\n",
    "            if BVAEtoDevice == True:\n",
    "                state_v = state_v.to(device)\n",
    "            #print(state_v.size(1)) # buffersize\n",
    "            state_v = BVAE.encode((state_v[0]).view(state_v.size(1), -1)) #preprocess with beta vae with bunch of 4\n",
    "            state_v = state_v.view(1, -1)\n",
    "          #  print(\"state_v size: \")\n",
    "          #  print(state_v.size())\n",
    "            \n",
    "            if BVAEtoDevice == False:\n",
    "                state_v = state_v.to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "           # print(\"q_vals_v size: \" ) \n",
    "           # print(q_vals_v.size())\n",
    "            \n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproBVAE(states_TOpreprocess):\n",
    "    #print(\"states_TOpreprocess size: \")\n",
    "    #print(states_TOpreprocess.size())\n",
    "\n",
    "    for i in range(states_TOpreprocess.size(0)):\n",
    "        states_processing = states_TOpreprocess[i]\n",
    "        #print(\"states_processing size: \")\n",
    "        #print(states_processing.size())\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        #print(\"viewed: \")\n",
    "        #print(states_processing.view(states_processing.size(0), -1).size())\n",
    "        temp = BVAE.encode(states_processing.view(states_processing.size(0), -1)) #preprocess with beta vae with bunch of 4\n",
    "        temp = temp[None, :] #expand by an axis [1, 128]\n",
    "        try:\n",
    "            states_preprocessed = torch.cat((temp , states_preprocessed), dim=0) #concatinate to finish tensor\n",
    "        except:\n",
    "            states_preprocessed = temp\n",
    "            \n",
    "    return states_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ipurwYpa6iKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2022-03-01 11:46:29.156064\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import math\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qEoc2PWmM2mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762:  101 games, mean reward -21.000, (epsilon 0.989)\n",
      "Best mean reward updated -21.000\n",
      "1680:  102 games, mean reward -20.900, (epsilon 0.975)\n",
      "Best mean reward updated -20.900\n",
      "2729:  103 games, mean reward -20.900, (epsilon 0.960)\n",
      "3704:  104 games, mean reward -20.800, (epsilon 0.946)\n",
      "Best mean reward updated -20.800\n",
      "4673:  105 games, mean reward -20.800, (epsilon 0.932)\n",
      "5503:  106 games, mean reward -20.800, (epsilon 0.921)\n",
      "6581:  107 games, mean reward -20.600, (epsilon 0.906)\n",
      "Best mean reward updated -20.600\n",
      "7431:  108 games, mean reward -20.600, (epsilon 0.895)\n",
      "8271:  109 games, mean reward -20.500, (epsilon 0.883)\n",
      "Best mean reward updated -20.500\n",
      "9052:  110 games, mean reward -20.500, (epsilon 0.873)\n",
      "10020:  111 games, mean reward -20.500, (epsilon 0.860)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3c3d87465bfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mstates_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproBVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_TOpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mnext_states_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproBVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states_TOpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-ba19ab961a2f>\u001b[0m in \u001b[0;36mpreproBVAE\u001b[1;34m(states_TOpreprocess)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(\"viewed: \")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(states_processing.view(states_processing.size(0), -1).size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBVAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#preprocess with beta vae with bunch of 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#expand by an axis [1, 128]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8a2f55425149>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "#*******Change************* That way every imprvement counts\n",
    "for i in range(100):\n",
    "    total_rewards.append(-21.000)\n",
    "\n",
    "frame_idx = 0  \n",
    "\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        \n",
    "        reward = agent.play_step(BVAE, net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            mean_reward = np.mean(total_rewards[-10:]) #changed from 100 to have a quicker downwards trend as well           \n",
    "            \n",
    "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.3f)\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "            \n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "        \n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        \n",
    "        #BVAE\n",
    "        states_TOpreprocess = torch.tensor(states)\n",
    "        next_states_TOpreprocess = torch.tensor(next_states)\n",
    "        \n",
    "        if BVAEtoDevice == True:\n",
    "            states_TOpreprocess = states_TOpreprocess.to(device)\n",
    "            next_states_TOpreprocess = next_states_TOpreprocess.to(device)\n",
    "        else:\n",
    "            states_TOpreprocess = states_TOpreprocess.to('cpu')\n",
    "            next_states_TOpreprocess = next_states_TOpreprocess.to('cpu')\n",
    "        \n",
    "        \n",
    "        states_preprocessed = preproBVAE(states_TOpreprocess)\n",
    "        next_states_preprocessed = preproBVAE(next_states_TOpreprocess)\n",
    "        \n",
    "        if BVAEtoDevice == False:\n",
    "            states_preprocessed = states_preprocessed.to(device)\n",
    "            next_states_preprocessed = next_states_preprocessed.to(device)\n",
    "        \n",
    "        \n",
    "        #print(\"states_preprocessed size: \")\n",
    "        #print(states_preprocessed.size())\n",
    "        \n",
    "        states_v = states_preprocessed.view(states_preprocessed.size(0), -1) #oder batchsize [batchsize, 4* features]\n",
    "        #print(\"states_v size: \")\n",
    "        #print(states_v.size())\n",
    "        next_states_v = next_states_preprocessed.view(next_states_preprocessed.size(0), -1)\n",
    "            \n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "        state_action_values = net(states_v).gather(1, actions_v.type(torch.int64).unsqueeze(-1)).squeeze(-1)\n",
    "        #For Linux use: state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "        next_state_values[done_mask] = 0.0\n",
    "\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        if frame_idx % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "       \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZPkszw66cmO"
   },
   "outputs": [],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKbcwfK321Hl"
   },
   "outputs": [],
   "source": [
    "tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "\n",
    "#!pip install pyvirtualdisplay==0.2.* \\\n",
    "#             PyOpenGL==3.1.* \\\n",
    "#             PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "#!pip install gym[box2d]==0.17.*\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvN4S8R53mJI"
   },
   "outputs": [],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"video\"  \n",
    "visualize=True\n",
    "\n",
    "env2 = make_env(DEFAULT_ENV_NAME)\n",
    "if record_folder:\n",
    "        env2 = gym.wrappers.Monitor(env2, record_folder, force=True)\n",
    "net = DQN(env2.observation_space.shape, env2.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env2.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env2.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env2.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
