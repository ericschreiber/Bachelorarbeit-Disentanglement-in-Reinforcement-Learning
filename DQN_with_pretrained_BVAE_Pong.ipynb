{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n",
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "bde4eb63-b316-46d9-cb3c-0d5b2f066da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8QDaXip14JBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1uzLQLz04z2i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VjUM99rEKFNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 29 10:03:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 497.09       Driver Version: 497.09       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A    0C    P8    N/A /  N/A |     37MiB /  2048MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "    \n",
    "class Scale01(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        obs[obs < 0.342] = 0\n",
    "        obs[:, 83, :] = 0 #unterste pixelreihe war immer 1 also hat keine Infos. Daher wird auf 0 gesetzt.\n",
    "        obs[obs > 0.1] = 1\n",
    "        return obs\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return Scale01(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "import torch.nn.functional as F\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "BVAEtoDevice = True #should prepro happen on GPU or CPU\n",
    "linear = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "features = 64\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "   #     self.conv = nn.Sequential(\n",
    "   #         nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "   #         nn.ReLU(),\n",
    "   #         nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "   #         nn.ReLU(),\n",
    "   #         nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "   #         nn.ReLU()\n",
    "   #     )\n",
    "        \n",
    "        \n",
    "        #conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.Linear(conv_out_size, 512),\n",
    "            nn.Linear(features*2*4, 512), #buffer von 4 Bilder nacheinander als input (features*2 kommt vom training vom VAE)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    #def _get_conv_out(self, shape):\n",
    "    #    o = self.conv(torch.zeros(1, *shape))\n",
    "    #    return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        #return self.fc(conv_out)\n",
    "        #print(x.size())\n",
    "        return self.fc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "taYi5LZnIOqz",
    "outputId": "c96d3a1d-ebf6-471a-a93f-c96b479cc9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained BVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 64\n",
    "# define a simple linear VAE #until now normal VAE without Beta\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearVAE, self).__init__()\n",
    " \n",
    "        # encoder 84*84 = 7’056\n",
    "        self.enc0 = nn.Linear(in_features=84*84, out_features=1024)\n",
    "        self.enc1 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.enc2 = nn.Linear(in_features=512, out_features=features*2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec0 = nn.Linear(in_features=features, out_features=512)\n",
    "        self.dec1 = nn.Linear(in_features=512, out_features=1024)\n",
    "        self.dec2 = nn.Linear(in_features=1024, out_features=84*84)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    " #   def forward(self, x):\n",
    " #       # encoding\n",
    " #       x = F.relu(self.enc0(x))\n",
    " #       x = F.relu(self.enc1(x))\n",
    "\n",
    " #       x = self.enc2(x).view(-1, 2, features)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    " #       mu = x[:, 0, :] # the first feature values as mean\n",
    " #       log_var = x[:, 1, :] # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    " #       z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    " #       x = F.relu(self.dec0(z))\n",
    " #       x = F.relu(self.dec1(x))\n",
    " #       reconstruction = torch.sigmoid(self.dec2(x))\n",
    " #       return reconstruction, mu, log_var\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.enc0(x))\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.enc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 42 x 42\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 1)  # 21 x 21\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 11 x 11\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 6 x 6\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(3136, 256) # 6x6x 64 = 2304\n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(\"conv1: \" + str(h.size()))\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(\"conv2: \" + str(h.size()))\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(\"conv3: \" + str(h.size()))\n",
    "        h = self.act(self.conv4(h))\n",
    "        #print(\"conv4: \" + str(h.size()))\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLikeDQN(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderLikeDQN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 10 x 10\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 10 x 10\n",
    "        self.flat1 = nn.Flatten()        \n",
    "        self.dense1 = nn.Linear(6400, 512) # 10x10x 64 = 6400\n",
    "        self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 3136)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, stride=2, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, stride=2, padding=1)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"decoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(\"Transpose 1: \" + str(h.size()))\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(\"Transpose 2: \" + str(h.size()))\n",
    "        h = self.act(self.upconv3(h))\n",
    "        #print(\"Transpose 3: \" + str(h.size()))\n",
    "        img = self.upconv4(h)\n",
    "        #print(\"Transpose 4: \" + str(img.size()))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLikeDQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderLikeDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        self.dense2 = nn.Linear(512, 6400)        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 10, 10)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        img = self.upconv3(h)\n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, device, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        #self.encoder = EncoderLikeDQN(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        #self.decoder = DecoderLikeDQN(z_dim, num_channels, latent_dim)\n",
    "        self.encoder = Encoder(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        self.decoder = Decoder(z_dim, num_channels, latent_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        self.mse = 0\n",
    "        self.bce = 0\n",
    "        self.tc = 0\n",
    "        self.to(device)\n",
    "        #self.rec_loss = nn.MSELoss() #try BCE Loss\n",
    "        self.rec_loss = nn.BCELoss()\n",
    "        #self.rec_loss = nn.BCEWithLogitsLoss() #clamp input values betweeen 0 & 1\n",
    "        \n",
    "        \n",
    "    def gaussian_log_density(self, z_sampled: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_logvar: TensorType[\"batch\", \"num_latents\"]):\n",
    "        normalization = torch.log(torch.tensor(2. * numpy.pi))\n",
    "        inv_sigma = torch.exp(-z_logvar)\n",
    "        tmp = (z_sampled - z_mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + z_logvar + normalization)    \n",
    "\n",
    "    def total_correlation(self, z: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_logvar: TensorType[\"batch\", \"num_latents\"]) -> torch.Tensor:\n",
    "    \n",
    "        batch_size = z.size(0)\n",
    "        log_qz_prob = self.gaussian_log_density(z.unsqueeze(1), z_mean.unsqueeze(0), z_logvar.unsqueeze(0))\n",
    "\n",
    "        log_qz_product = torch.sum(\n",
    "            torch.logsumexp(log_qz_prob, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        log_qz = torch.logsumexp(\n",
    "            torch.sum(log_qz_prob, dim=2),\n",
    "            dim=1\n",
    "        )\n",
    "        return torch.mean(log_qz - log_qz_product)\n",
    "\n",
    "    \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "       \n",
    "        \n",
    "    def num_channels(self):\n",
    "        return self.encoder.num_channels\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)\n",
    "        if torch.isnan(z).any():\n",
    "            print(\"z has NaN\")\n",
    "            print(z)\n",
    "            print(\"*************************************input saved***********\")\n",
    "            x = x.cpu().detach().numpy()\n",
    "            numpy.save( \"faulty_batch\", x)\n",
    "\n",
    "            \n",
    "            \n",
    "        mu = z[:, :, 0]\n",
    "        logvar = z[:, :, 1]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        reparam_z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = 0.5 * (sigma**2 + mu**2 - 2*torch.log(sigma) - 1).mean()\n",
    "        self.tc = self.total_correlation(reparam_z, mu, logvar)\n",
    "        \n",
    "        x_t = self.decoder(reparam_z).sigmoid()\n",
    "        #if torch.isnan(x_t).any():\n",
    "            #print(x_t)\n",
    "        #pred = x_t.clamp(0, 1) #push values between 0 and 1\n",
    "        #pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred) #vlt muss das noch rein\n",
    "        \n",
    "        #self.mse = self.rec_loss(x_t, x)\n",
    "        self.bce = self.rec_loss(x_t, x)\n",
    "        return x_t\n",
    "    \n",
    "    # TODO: Passe diese Klasse noch an. Vlt geht damit das Kopieren zurück\n",
    "    \n",
    "    def encode(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)            \n",
    "        mu = z[:, :, 0]\n",
    "        logvar = z[:, :, 1]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        z[:, :, 1] = sigma\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3136, out_features=256, bias=True)\n",
      "    (dense_means_logVar): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (dense2): Linear(in_features=256, out_features=3136, bias=True)\n",
      "    (upconv1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv3): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (rec_loss): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if linear == False:\n",
    "    latentDim = 64 #für conv model\n",
    "    BVAE = VAE(latentDim, 1, device, latentDim)\n",
    "    BVAE.load_state_dict(torch.load('C:/Users/erics/Documents/Programme/Bachelorarbeit/models/BTCVAE_Pong/ConvTC0.0001_Beta3Lat64lr0.0001-bestMAR23.dat'))\n",
    "    print(BVAE)\n",
    "else:\n",
    "    BVAE = LinearVAE()\n",
    "    BVAE.load_state_dict(torch.load('C:/Users/erics/Documents/Programme/Bachelorarbeit/models/BVAE_Pong/B=10VAEFEB25'))\n",
    "\n",
    "if BVAEtoDevice == True:\n",
    "    BVAE.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "VISUALIZEtraining = False\n",
    "MEAN_REWARD_BOUND = -19   #Change to 19.0    \n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, BVAE, net, epsilon=0.0, device=\"cpu\"):\n",
    "        \n",
    "        if VISUALIZEtraining:\n",
    "            env.render()\n",
    "            \n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a)\n",
    "            if BVAEtoDevice == True:\n",
    "                state_v = state_v.to(device)\n",
    "            #print(state_v.size(1)) # buffersize\n",
    "            if linear == True:\n",
    "                state_v = BVAE.encode((state_v[0]).view(state_v.size(1), -1)) #preprocess with beta vae with bunch of 4\n",
    "                state_v = state_v.view(1, -1)\n",
    "            else:\n",
    "                state_v = state_v.view(4, 1, state_v.size(2), -1)\n",
    "                state_v = BVAE.encode((state_v))\n",
    "                state_v = state_v.view(-1)\n",
    "                \n",
    "            #print(\"state_v size: \")\n",
    "            #print(state_v.size())\n",
    "            \n",
    "            if BVAEtoDevice == False:\n",
    "                state_v = state_v.to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "           # print(\"q_vals_v size: \" ) \n",
    "           # print(q_vals_v.size())\n",
    "            \n",
    "            # _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            _, act_v = torch.max(q_vals_v, dim=0)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproBVAElinear(states_TOpreprocess):\n",
    "    #print(\"states_TOpreprocess size: \")\n",
    "    #print(states_TOpreprocess.size())\n",
    "\n",
    "    for i in range(states_TOpreprocess.size(0)):\n",
    "        states_processing = states_TOpreprocess[i]\n",
    "        #print(\"states_processing size: \")\n",
    "        #print(states_processing.size())\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        #print(\"viewed: \")\n",
    "        #print(states_processing.view(states_processing.size(0), -1).size())\n",
    "        temp = BVAE.encode(states_processing.view(states_processing.size(0), -1)) #preprocess with beta vae with bunch of 4\n",
    "        temp = temp[None, :] #expand by an axis [1, 128]\n",
    "        try:\n",
    "            states_preprocessed = torch.cat((temp , states_preprocessed), dim=0) #concatinate to finish tensor\n",
    "        except:\n",
    "            states_preprocessed = temp\n",
    "            \n",
    "    return states_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ipurwYpa6iKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2022-03-29 10:04:04.509236\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import math\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qEoc2PWmM2mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781:  101 games, mean reward -21.000, (epsilon 0.988)\n",
      "Best mean reward updated -21.000\n",
      "1820:  102 games, mean reward -20.800, (epsilon 0.973)\n",
      "Best mean reward updated -20.800\n",
      "2755:  103 games, mean reward -20.600, (epsilon 0.960)\n",
      "Best mean reward updated -20.600\n",
      "3655:  104 games, mean reward -20.500, (epsilon 0.947)\n",
      "Best mean reward updated -20.500\n",
      "4417:  105 games, mean reward -20.500, (epsilon 0.936)\n",
      "5405:  106 games, mean reward -20.400, (epsilon 0.922)\n",
      "Best mean reward updated -20.400\n",
      "6293:  107 games, mean reward -20.300, (epsilon 0.910)\n",
      "Best mean reward updated -20.300\n",
      "7358:  108 games, mean reward -20.200, (epsilon 0.896)\n",
      "Best mean reward updated -20.200\n",
      "8319:  109 games, mean reward -20.200, (epsilon 0.883)\n",
      "9207:  110 games, mean reward -20.200, (epsilon 0.871)\n",
      "10070:  111 games, mean reward -20.100, (epsilon 0.860)\n",
      "Best mean reward updated -20.100\n",
      "10939:  112 games, mean reward -20.300, (epsilon 0.849)\n",
      "11897:  113 games, mean reward -20.400, (epsilon 0.837)\n",
      "12765:  114 games, mean reward -20.400, (epsilon 0.826)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-5734432f5bf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBVAE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-30c9f1ab946e>\u001b[0m in \u001b[0;36mplay_step\u001b[1;34m(self, BVAE, net, epsilon, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mstate_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mBVAEtoDevice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mstate_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;31m#print(state_v.size(1)) # buffersize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlinear\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "#*******Change************* That way every imprvement counts\n",
    "for i in range(100):\n",
    "    total_rewards.append(-21.000)\n",
    "\n",
    "frame_idx = 0  \n",
    "\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        \n",
    "        reward = agent.play_step(BVAE, net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            mean_reward = np.mean(total_rewards[-10:]) #changed from 100 to have a quicker downwards trend as well           \n",
    "            \n",
    "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.3f)\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "            \n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "        \n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        \n",
    "        #BVAE\n",
    "        states_TOpreprocess = torch.tensor(states)\n",
    "        next_states_TOpreprocess = torch.tensor(next_states)\n",
    "        \n",
    "\n",
    "        if BVAEtoDevice == True:\n",
    "            states_TOpreprocess = states_TOpreprocess.to(device)\n",
    "            next_states_TOpreprocess = next_states_TOpreprocess.to(device)\n",
    "        else:\n",
    "            states_TOpreprocess = states_TOpreprocess.to('cpu')\n",
    "            next_states_TOpreprocess = next_states_TOpreprocess.to('cpu')\n",
    "\n",
    "        if linear == True:\n",
    "            states_preprocessed = preproBVAElinear(states_TOpreprocess)\n",
    "            next_states_preprocessed = preproBVAElinear(next_states_TOpreprocess)\n",
    "\n",
    "        else: #conv layers\n",
    "            #Ich weiss nicht ob die BIlder so in der korrekten Reihenfolge sind oder die 4 Bilder in Blöcken von 32 nacheinander\n",
    "            states_TOpreprocess = states_TOpreprocess.view(-1, 1, states_TOpreprocess.size(2), states_TOpreprocess.size(3))\n",
    "            next_states_TOpreprocess = next_states_TOpreprocess.view(-1, 1, next_states_TOpreprocess.size(2), next_states_TOpreprocess.size(3))\n",
    "            states_preprocessed = BVAE.encode(states_TOpreprocess)\n",
    "            next_states_preprocessed = BVAE.encode(next_states_TOpreprocess)\n",
    "            #Wieder dientanglen: (batchsize, Buffer, 84, 84)\n",
    "            states_preprocessed = states_preprocessed.view(-1, 4, states_preprocessed.size(1), states_preprocessed.size(2))\n",
    "            next_states_preprocessed = next_states_preprocessed.view(-1, 4, next_states_preprocessed.size(1), next_states_preprocessed.size(2))\n",
    "            #print(\"states_preprocessed size: \")\n",
    "            #print(states_preprocessed.size())\n",
    "            \n",
    "            \n",
    "            \n",
    "        if BVAEtoDevice == False:\n",
    "            states_preprocessed = states_preprocessed.to(device)\n",
    "            next_states_preprocessed = next_states_preprocessed.to(device)\n",
    "\n",
    "        #print(\"states_preprocessed size: \")\n",
    "        #print(states_preprocessed.size())\n",
    "        \n",
    "        states_v = states_preprocessed.view(states_preprocessed.size(0), -1) #oder batchsize [batchsize, 4* features]\n",
    "        #print(\"states_v size: \")\n",
    "        #print(states_v.size())\n",
    "        next_states_v = next_states_preprocessed.view(next_states_preprocessed.size(0), -1)\n",
    "            \n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "        state_action_values = net(states_v).gather(1, actions_v.type(torch.int64).unsqueeze(-1)).squeeze(-1)\n",
    "        #For Linux use: state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "        next_state_values[done_mask] = 0.0\n",
    "\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        if frame_idx % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "       \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZPkszw66cmO"
   },
   "outputs": [],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKbcwfK321Hl"
   },
   "outputs": [],
   "source": [
    "tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "\n",
    "#!pip install pyvirtualdisplay==0.2.* \\\n",
    "#             PyOpenGL==3.1.* \\\n",
    "#             PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "#!pip install gym[box2d]==0.17.*\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvN4S8R53mJI"
   },
   "outputs": [],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"video\"  \n",
    "visualize=True\n",
    "\n",
    "env2 = make_env(DEFAULT_ENV_NAME)\n",
    "if record_folder:\n",
    "        env2 = gym.wrappers.Monitor(env2, record_folder, force=True)\n",
    "net = DQN(env2.observation_space.shape, env2.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env2.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env2.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env2.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
