{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875249a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd44782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.profiler\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "torch.set_printoptions(profile=\"full\") #print full tensor\n",
    "#torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9505b345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15ecfb1def0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 0 \n",
    "numpy.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.random.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 42 x 42\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 1)  # 21 x 21\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 11 x 11\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 6 x 6\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(3136, 256) # 6x6x 64 = 2304\n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(\"conv1: \" + str(h.size()))\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(\"conv2: \" + str(h.size()))\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(\"conv3: \" + str(h.size()))\n",
    "        h = self.act(self.conv4(h))\n",
    "        #print(\"conv4: \" + str(h.size()))\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6180178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLikeDQN(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderLikeDQN, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 20 x 20\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 10 x 10\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 10 x 10\n",
    "        self.flat1 = nn.Flatten()        \n",
    "        self.dense1 = nn.Linear(6400, 512) # 10x10x 64 = 6400\n",
    "        self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        #print(x.size())\n",
    "        h = self.act(self.conv1(x))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.conv3(h))\n",
    "        #print(h.size())\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b28a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderL1(nn.Module):\n",
    "    def __init__(self, output_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(EncoderL1, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        #self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0)  # 41 x 41\n",
    "        #self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 20 x 20 \n",
    "        #self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)  # 20 x 20\n",
    "        #self.flat1 = nn.Flatten()        \n",
    "        #self.dense1 = nn.Linear(25600, 4096) #\n",
    "        #self.dense2 = nn.Linear(4096, 512) #\n",
    "        #self.dense_means_logVar = nn.Linear(512, latent_dim*2)\n",
    "        #self.dense_log_var = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2, padding=1)  # 84 x 84\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2, 2, 0)  # 43 x 43\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, 2, 1)  # 22 x 22\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2, 2, 1)  # 12 x 12\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(9216, 3136) # 12x12x 64 = 9216\n",
    "        self.dense2 = nn.Linear(3136, 256) \n",
    "        self.dense_means_logVar = nn.Linear(256, latent_dim*2)\n",
    "        \n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> (TensorType[\"batch\", \"output_dim\"], TensorType[\"batch\", \"output_dim\"]):\n",
    "        #print(\"encoder: \")\n",
    "        print(f\"inputsize: {x.size()}\")\n",
    "        h = self.act(self.conv1(x))\n",
    "        print(f\"conv 1: {h.size()}\")\n",
    "        h = self.act(self.conv2(h))\n",
    "        print(f\"conv 2: {h.size()}\")\n",
    "        h = self.act(self.conv3(h))\n",
    "        print(f\"conv 3: {h.size()}\")\n",
    "        h = self.act(self.conv4(h))\n",
    "        print(f\"conv 4: {h.size()}\")\n",
    "        \n",
    "        h = self.flat1(h)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.dense1(h))\n",
    "        h = self.act(self.dense2(h))\n",
    "        #print(h.size())\n",
    "        #means = self.dense_means(h)\n",
    "        #print(means.size())\n",
    "        #log_var = self.dense_log_var(h)\n",
    "        #print(log_var.size())\n",
    "        \n",
    "        return self.dense_means_logVar(h)\n",
    "        \n",
    "        #sample = self.reparameterize(means, log_var)\n",
    "        \n",
    "        #return sample, means, log_var\n",
    "        #return means, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c85bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 3136)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, stride=2, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, stride=2, padding=1)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"decoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(\"Transpose 1: \" + str(h.size()))\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(\"Transpose 2: \" + str(h.size()))\n",
    "        h = self.act(self.upconv3(h))\n",
    "        #print(\"Transpose 3: \" + str(h.size()))\n",
    "        img = self.upconv4(h)\n",
    "        #print(\"Transpose 4: \" + str(img.size()))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e9f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLikeDQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderLikeDQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        self.dense2 = nn.Linear(512, 6400)        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = h.view(-1, 64, 10, 10)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        img = self.upconv3(h)\n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f577568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderL1(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_channels: int, latent_dim: int):\n",
    "        super(DecoderL1, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.dense1 = nn.Linear(latent_dim, 512)\n",
    "        #self.dense2 = nn.Linear(512, 4096)\n",
    "        #self.dense3 = nn.Linear(4096, 25600)        \n",
    "        self.dense1 = nn.Linear(latent_dim, 256)\n",
    "        self.dense2 = nn.Linear(256, 3136)\n",
    "        self.dense3 = nn.Linear(3136, 9216)  \n",
    "        \n",
    "        #self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        #self.upconv2 = nn.ConvTranspose2d(32, 32, 4, 2, 1)\n",
    "        #self.upconv3 = nn.ConvTranspose2d(32, num_channels, 8, 4, 0)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 32, 2, stride=2, padding=0)\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, num_channels, 4, stride=2, padding=1)\n",
    "        \n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, z: TensorType[\"batch\", \"input_dim\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        #print(\"encoder: \")\n",
    "        h = self.act(self.dense1(z))\n",
    "        h = self.act(self.dense2(h))\n",
    "        h = self.act(self.dense3(h))\n",
    "        #h = h.view(-1, 64, 20, 20)\n",
    "        h = h.view(-1, 64, 12, 12)\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv1(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv2(h))\n",
    "        #print(h.size())\n",
    "        h = self.act(self.upconv3(h))\n",
    "        img = self.upconv4(h)        \n",
    "        #print(img.size())\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b8a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, device, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        self.encoder = EncoderLikeDQN(z_dim, num_channels, latent_dim) # use \"wrong\" encoder\n",
    "        self.decoder = DecoderLikeDQN(z_dim, num_channels, latent_dim)\n",
    "        #self.encoder = Encoder(z_dim, num_channels, latent_dim) \n",
    "        #self.decoder = Decoder(z_dim, num_channels, latent_dim)\n",
    "        #self.encoder = EncoderL1(z_dim, num_channels, latent_dim) \n",
    "        #self.decoder = DecoderL1(z_dim, num_channels, latent_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device) # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0\n",
    "        self.mse = 0\n",
    "        self.bce = 0\n",
    "        self.tc = 0\n",
    "        self.to(device)\n",
    "        #self.rec_loss = nn.MSELoss() #try BCE Loss\n",
    "        #self.rec_loss = nn.BCELoss()\n",
    "        self.rec_loss = nn.BCEWithLogitsLoss() #clamp input values betweeen 0 & 1 use without sigmoid after last output\n",
    "        \n",
    "        \n",
    "    def gaussian_log_density(self, z_sampled: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                         z_logvar: TensorType[\"batch\", \"num_latents\"]):\n",
    "        normalization = torch.log(torch.tensor(2. * numpy.pi))\n",
    "        inv_sigma = torch.exp(-z_logvar)\n",
    "        tmp = (z_sampled - z_mean)\n",
    "        return -0.5 * (tmp * tmp * inv_sigma + z_logvar + normalization)    \n",
    "\n",
    "    def total_correlation(self, z: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_mean: TensorType[\"batch\", \"num_latents\"],\n",
    "                      z_logvar: TensorType[\"batch\", \"num_latents\"]) -> torch.Tensor:\n",
    "    \n",
    "        batch_size = z.size(0)\n",
    "        log_qz_prob = self.gaussian_log_density(z.unsqueeze(1), z_mean.unsqueeze(0), z_logvar.unsqueeze(0))\n",
    "\n",
    "        log_qz_product = torch.sum(\n",
    "            torch.logsumexp(log_qz_prob, dim=1),\n",
    "            dim=1\n",
    "        )\n",
    "        log_qz = torch.logsumexp(\n",
    "            torch.sum(log_qz_prob, dim=2),\n",
    "            dim=1\n",
    "        )\n",
    "        return torch.mean(log_qz - log_qz_product)\n",
    "\n",
    "    \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "       \n",
    "        \n",
    "    def num_channels(self):\n",
    "        return self.encoder.num_channels\n",
    "    \n",
    "    def decode(self, reparam_z):\n",
    "        \n",
    "        return self.decoder(reparam_z)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n",
    "                ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n",
    "        z = self.encoder(x).view(x.size(0), self.z_dim, 2)\n",
    "        if torch.isnan(z).any():\n",
    "            print(\"z has NaN\")\n",
    "            print(z)\n",
    "            print(\"*************************************input saved***********\")\n",
    "            x = x.cpu().detach().numpy()\n",
    "            numpy.save( \"faulty_batch\", x)\n",
    "\n",
    "            \n",
    "            \n",
    "        mu = z[:, :, 0]\n",
    "        logvar = z[:, :, 1]\n",
    "        sigma = torch.exp(z[:, :, 1])\n",
    "        reparam_z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = 0.5 * (sigma**2 + mu**2 - 2*torch.log(sigma) - 1).mean()\n",
    "        self.tc = self.total_correlation(reparam_z, mu, logvar)\n",
    "        \n",
    "        #x_t = self.decoder(reparam_z).sigmoid()\n",
    "        x_t = self.decoder(reparam_z) #No sigmoid if BCEWithLogitsLoss\n",
    "        \n",
    "        #if torch.isnan(x_t).any():\n",
    "            #print(x_t)\n",
    "        #pred = x_t.clamp(0, 1) #push values between 0 and 1\n",
    "        #pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred) #vlt muss das noch rein\n",
    "        \n",
    "        #self.mse = self.rec_loss(x_t, x)\n",
    "        self.bce = self.rec_loss(x_t, x)\n",
    "        return mu, logvar, x_t\n",
    "    \n",
    "    # TODO: Passe diese Klasse noch an. Vlt geht damit das Kopieren zurück"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb022fe",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb50a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numpy.load('train_data100kMAR22.npy')\n",
    "val_data = numpy.load('val_data20kMAR22.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f23c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numpy.load(r\"C:\\Users\\erics\\Documents\\Programme\\Bachelorarbeit\\L1train_data100kWithLabels.npz\")['data']\n",
    "val_data = numpy.load(r\"C:\\Users\\erics\\Documents\\Programme\\Bachelorarbeit\\L1val_data20kBWithLabels.npz\")['data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c905dc",
   "metadata": {},
   "source": [
    "Downsampling the Data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dc3b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2=train_data[:, 0::2,0::2] #downsample\n",
    "val_data2=val_data[:, 0::2,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4186bb30",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data2[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d096993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 168, 168])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erics\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import interpolate\n",
    "train_dataPy= torch.from_numpy(train_data)\n",
    "train_dataPy= train_dataPy[:, None, :, :]\n",
    "print(train_dataPy.size())\n",
    "train_data3 = interpolate(train_dataPy, scale_factor=(0.5,0.5), mode='bilinear')\n",
    "\n",
    "del train_dataPy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5faa26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "train_data31 = train_data3.squeeze()\n",
    "print(train_data31.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ce4ba6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data31[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c95659",
   "metadata": {},
   "source": [
    "Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b492bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "latentDim = 64\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "kl_wheight = 0.00064\n",
    "beta = 3\n",
    "\n",
    "if beta != 0 :\n",
    "    tc_wheight = beta - 1\n",
    "else: \n",
    "    tc_wheight = 0\n",
    "beta = 1\n",
    "\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fedd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runLIKEDQNBeta{beta}Lat{latentDim}\"\n",
    "#newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/run1Beta{beta}Lat{latentDim}\"\n",
    "newpath = f\"C:/Users/erics/Documents/Programme/Bachelorarbeit/beat_VAE_Pong_runs/runL1Lat{latentDim}\"\n",
    "newpath = newpath + \"/output{x.day}-{x.month}\"\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "savingDir = newpath + \"/epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fa21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True, #this instructs DataLoader to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU.\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a960ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): EncoderL1(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (flat1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=9216, out_features=3136, bias=True)\n",
      "    (dense2): Linear(in_features=3136, out_features=256, bias=True)\n",
      "    (dense_means_logVar): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): DecoderL1(\n",
      "    (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (dense2): Linear(in_features=256, out_features=3136, bias=True)\n",
      "    (dense3): Linear(in_features=3136, out_features=9216, bias=True)\n",
      "    (upconv1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (upconv3): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (upconv4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (rec_loss): BCEWithLogitsLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#enc = Encoder(latentDim, 1, latentDim).to(device)\n",
    "#dec = Decoder(latentDim, 1, latentDim).to(device)\n",
    "#optEnc = optim.Adam(enc.parameters(), lr=lr)\n",
    "#optDec = optim.Adam(dec.parameters(), lr=lr)\n",
    "\n",
    "#print(enc)\n",
    "#print(dec)\n",
    "\n",
    "vae = VAE(latentDim, 1, device, latentDim).to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "#criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52bb8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar, beta, kl_wheight):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the (one could also take the mse loss instead of bce then we get a kind of PCA)\n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta*kl_wheight*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8fb6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e34130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_generator_jacobian_image_optimized(model, embedding, epsilon_scale = 0.001, device=\"cpu\"):\n",
    "    raw_jacobian = compute_generator_jacobian_optimized(model, embedding, epsilon_scale, device)\n",
    "    # shape is (latent_size, batch_size, numchannels = 1, im_size, im_size)\n",
    "    jacobian = torch.sum(raw_jacobian, dim=2,keepdim = True)\n",
    "    return(jacobian)\n",
    "\n",
    "# output shape is (latent_dim, batch_size, model_output_shape)\n",
    "def compute_generator_jacobian_optimized(model, embedding, epsilon_scale = 0.001, device=\"cpu\"):\n",
    "    batch_size = embedding.shape[0]\n",
    "    latent_dim = embedding.shape[1]\n",
    "    # repeat \"tiles\" like ABCABCABC (not AAABBBCCC)\n",
    "    # note that we detach the embedding here, so we should hopefully\n",
    "    # not be pulling our gradients further back than we intend\n",
    "    encoding_rep = embedding.repeat(latent_dim + 1,1).detach().clone() #why hier kopie??\n",
    "    # define our own repeat to work like \"AAABBBCCC\"\n",
    "    delta = torch.eye(latent_dim)\\\n",
    "                .reshape(latent_dim, 1, latent_dim)\\\n",
    "                .repeat(1, batch_size, 1)\\\n",
    "                .reshape(latent_dim*batch_size, latent_dim)\n",
    "    delta = torch.cat((delta, torch.zeros(batch_size,latent_dim))).to(device)\n",
    "    # we randomized this before up to epsilon_scale,\n",
    "    # but for now let's simplify and just have this equal to epsilon_scale.\n",
    "    # I'd be _very_ impressed if the network can figure out to make the results\n",
    "    # periodic with this frequency in order to get around this gradient check.\n",
    "    epsilon = epsilon_scale     \n",
    "    encoding_rep += epsilon * delta\n",
    "    recons = model.decode(encoding_rep)\n",
    "    temp_calc_shape = [latent_dim+1,batch_size] + list(recons.shape[1:])\n",
    "    recons = recons.reshape(temp_calc_shape)\n",
    "    recons = (recons[:-1] - recons[-1])/epsilon\n",
    "    return(recons)\n",
    "\n",
    "\n",
    "def jacobian_loss_function(model, mu, logvar, device):\n",
    "    # jacobian shape is (latent_dim, batch_size, output_model_shape)\n",
    "    # where the first batch_size rows correspond to the first latent dimension, etc.\n",
    "    jacobian = compute_generator_jacobian_optimized(model, mu, epsilon_scale = 0.001, device=device)\n",
    "    #print(jacobian.shape)\n",
    "    latent_dim = jacobian.shape[0]\n",
    "    batch_size = jacobian.shape[1]\n",
    "    jacobian = jacobian.reshape((latent_dim, batch_size, -1))\n",
    "    obs_dim = jacobian.shape[2]\n",
    "    loss = torch.sum(torch.abs(jacobian))/batch_size\n",
    "    assert len(loss.shape)==0, \"loss should be a scalar\"\n",
    "    return(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ce79f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "775e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fit(enc, dec, dataloader):\n",
    "def fit(vae, dataloader):\n",
    "    #enc.train()\n",
    "    #dec.train()\n",
    "    vae.train\n",
    "    running_loss = 0.0\n",
    "   # with torch.profiler.profile(schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=10),\n",
    "   #                             on_trace_ready=torch.profiler.tensorboard_trace_handler('C:/Users/erics/Documents/Programme/Bachelorarbeit/Profiler/BVAE/bestermann_MAR9_VAE_Class_Run8_runningLoss/'),\n",
    "   #                             record_shapes=True,\n",
    "   #                             profile_memory=True,\n",
    "   #                             with_stack=True) as prof: \n",
    "        \n",
    "   #     prof.start()\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        data = data.to(device)\n",
    "        data = data[:, None, :, :]\n",
    "\n",
    "        #optEnc.zero_grad()\n",
    "        #optDec.zero_grad()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        #opt.zero_grad()\n",
    "\n",
    "      #  interData = enc(data)\n",
    "      #  sample, mu, logvar = interData\n",
    "        #interData = reparameterize(mu, logvar)\n",
    "      #  reconstruction = dec(sample)\n",
    "        mu, logvar, reconstruction = vae(data)        \n",
    "\n",
    "        #mse_loss = criterion(reconstruction, data)\n",
    "        #loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(train_data))\n",
    "        #kl_wheight = dataloader.batch_size/len(train_data) fixer hyperparameter\n",
    "        #loss = kl_wheight * beta * vae.kl + vae.mse\n",
    "        loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "        loss += jacobian_loss_function(vae, mu, logvar, device)\n",
    "\n",
    "        #running_loss += loss.item()\n",
    "        running_loss += loss.detach().cpu().numpy() # faster with detach().cpu().numpy() but double the copied amount just for plotting purposes\n",
    "        loss.backward()\n",
    "      #  optEnc.step()\n",
    "      #  optDec.step()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.5, norm_type=2.0) #args.clip) #clipping gradient\n",
    "        opt.step()\n",
    "        #print(opt.param_groups[0]['lr'])\n",
    "\n",
    "       #     prof.step()\n",
    "       #     if(i > 100):\n",
    "       #         break\n",
    "       # prof.stop()\n",
    "        \n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316b4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def validate(enc, dec, dataloader):\n",
    "def validate(vae, dataloader):\n",
    "    #enc.eval()\n",
    "    #dec.eval()\n",
    "    vae.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data = data.to(device)\n",
    "            data = data[:, None, :, :]\n",
    "            \n",
    "          #  interData = enc(data)\n",
    "          #  sample, mu, logvar = interData\n",
    "          #  reconstruction = dec(sample)\n",
    "                \n",
    "          #  mse_loss = criterion(reconstruction, data)\n",
    "          #  loss = final_loss(mse_loss, mu, logvar, beta, kl_wheight = dataloader.batch_size/len(val_data))\n",
    "          #  running_loss += loss.item()\n",
    "            mu, logvar, reconstruction = vae(data)        \n",
    "            loss = kl_wheight * beta * vae.kl + vae.bce + tc_wheight * vae.tc\n",
    "            loss += jacobian_loss_function(vae, mu, logvar, device)\n",
    "            \n",
    "            running_loss += loss.detach().cpu()\n",
    "            \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 1, 84, 84)[:8], \n",
    "                                  reconstruction.view(batch_size, 1, 84, 84)[:8]))\n",
    "                save_image(both.cpu(), savingDir + f\"{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adcba3ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputsize: torch.Size([64, 1, 84, 168])\n",
      "conv 1: torch.Size([64, 32, 42, 84])\n",
      "conv 2: torch.Size([64, 32, 21, 42])\n",
      "conv 3: torch.Size([64, 64, 11, 22])\n",
      "conv 4: torch.Size([64, 64, 6, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/234 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x4608 and 9216x3136)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8026ac15e87d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#val_epoch_loss = validate(enc, dec, val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-e86019861d05>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(vae, dataloader)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#interData = reparameterize(mu, logvar)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[1;31m#  reconstruction = dec(sample)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#mse_loss = criterion(reconstruction, data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-4e3f3dc77dbe>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m     def forward(self, x: TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]\n\u001b[0;32m     72\u001b[0m                 ) -> TensorType[\"batch\", \"num_channels\", \"x\", \"y\"]:\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"z has NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-99aa0a6b4361>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#print(h.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#print(h.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PyTorchRL\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x4608 and 9216x3136)"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "torch.backends.cudnn.benchmark = True #choose best kernel for computation\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    #train_epoch_loss = fit(enc, dec, train_loader)\n",
    "    #val_epoch_loss = validate(enc, dec, val_loader)\n",
    "    \n",
    "    train_epoch_loss = fit(vae, train_loader)\n",
    "    val_epoch_loss = validate(vae, val_loader)\n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0aac4",
   "metadata": {},
   "source": [
    "* Auslastung GPU: copy ~90%, vram 100% ( 2GB), 3D 0%\n",
    "* Auslastung CPU: ~25 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57af84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67aa871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_batch = numpy.load('faulty_batch.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db27d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "    all_zeros = not faulty_batch[i].any()\n",
    "    print(all_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f352bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(faulty_batch[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
